<div align="center">

# ğŸŒ€ S.O.N.A.R

### **Sonic Oracle for Navigating Ancient Ruins**

*Listening to the past, one frequency at a time*

[![Kaggle](https://img.shields.io/badge/Kaggle-Competition-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/competitions/openai-to-z-challenge)
[![Notebook](https://img.shields.io/badge/Notebook-View-FF6F00?style=for-the-badge&logo=jupyter&logoColor=white)](https://www.kaggle.com/code/arnavmishra6996/whispers-beneath-the-canopy)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)

---

</div>

## ğŸ¯ What is SONAR?

**SONAR** is a groundbreaking AI system that "listens" to the Amazon rainforest to discover hidden archaeological sites. By converting satellite data, LiDAR terrain models, and hydrological patterns into sound, it uses machine learning to detect acoustic anomalies that reveal ancient structures invisible to traditional remote sensing.

<div align="center">

### ğŸ”Š The Innovation

| Traditional Approach | SONAR Approach |
|:-------------------:|:--------------:|
| ğŸ‘ï¸ Visual Analysis | ğŸ‘‚ Acoustic Analysis |
| Limited by canopy | Penetrates dense vegetation |
| Manual interpretation | AI-driven detection |
| 2D/3D spatial data | Multi-dimensional soundscapes |

</div>

---

## ğŸŒ How It Works

<div align="center">

```mermaid
graph LR
    A[ğŸ›°ï¸ Geospatial Data] --> B[ğŸµ Sonification]
    B --> C[ğŸ§  Audio Embeddings]
    C --> D[ğŸ” Anomaly Detection]
    D --> E[ğŸ›ï¸ Archaeological Sites]
    
    style A fill:#667eea
    style B fill:#764ba2
    style C fill:#f093fb
    style D fill:#4facfe
    style E fill:#43e97b
```

</div>

### The Pipeline

<table>
<tr>
<td width="20%" align="center">

### ğŸ¨ Step 1
**Sonification**

Convert terrain elevation, vegetation density, and water patterns into layered audio

</td>
<td width="20%" align="center">

### ğŸ¼ Step 2
**Audio Embedding**

Extract 128-dim features using VGGish neural network

</td>
<td width="20%" align="center">

### ğŸ” Step 3
**Anomaly Detection**

Isolation Forest finds unusual acoustic patterns

</td>
<td width="20%" align="center">

### ğŸ¯ Step 4
**Pattern Matching**

DTW algorithm classifies archaeological features

</td>
<td width="20%" align="center">

### ğŸ—ºï¸ Step 5
**Visualization**

Interactive maps display discoveries

</td>
</tr>
</table>

---

## ğŸ§© Technical Architecture

### **Module 1: Geospatial Sonification Engine** ğŸµ

<details>
<summary><b>Click to expand technical details</b></summary>

<br>

**Input Data:**
- ğŸŒ **LiDAR DTMs** â€” 1m resolution digital terrain models
- ğŸ›°ï¸ **Sentinel-2 Imagery** â€” Multi-spectral satellite data (NDVI, NDWI)
- ğŸ’§ **HydroSHEDS** â€” Flow direction, accumulation patterns

**Data-to-Sound Mapping:**

| Geospatial Feature | Audio Parameter | Effect |
|-------------------|-----------------|--------|
| Elevation | Bass Frequency | Low = valleys, High = peaks |
| Vegetation (NDVI) | Melodic Tones | Dense = rich harmonics |
| Terrain Slope | Rhythm Pattern | Steep = fast tempo |
| Water (NDWI) | Texture Layer | Present = reverb effect |

**Output:**
- `{transect}_full_sonification_SOTA.wav` â€” Complete audio landscape
- `{transect}_geospatial_metadata.json` â€” Audio-coordinate mapping
- `{transect}_visualization.png` â€” Visual representation

</details>

<div align="center">
  <img src="./images/Screenshot%202025-10-26%20031458.png" alt="Geospatial Sonification Process" width="750"/>
  <p><em>ğŸ¨ LiDAR elevation data transformed into acoustic landscape with archaeological anomaly markers</em></p>
</div>

---

### **Module 2: Sonic Feature Embedding (VGGish)** ğŸ§ 

<details>
<summary><b>Click to expand technical details</b></summary>

<br>

**Model:** [Google VGGish](https://tfhub.dev/google/vggish/1) â€” Pre-trained on AudioSet (2M+ audio samples)

**Processing Pipeline:**

```python
Audio Stream â†’ Chunk (10s) â†’ Resample (16kHz) â†’ VGGish â†’ 128-dim Embeddings
```

**Key Features:**
- âš¡ Memory-efficient streaming
- ğŸ¯ 0.96s temporal resolution
- ğŸ“Š 128-dimensional feature space
- ğŸ”„ Transfer learning from AudioSet

**Output:** `audio_embeddings/{transect}_embeddings.npy`

</details>

---

### **Module 3: Sonic Anomaly Detection (Isolation Forest)** ğŸ”

<details>
<summary><b>Click to expand technical details</b></summary>

<br>

**Training Strategy:**

> ğŸ’¡ Train only on "normal" terrain â†’ Archaeological sites emerge as outliers

**Algorithm Flow:**

1. **Baseline Training** â€” Learn typical landscape acoustics
2. **Anomaly Scoring** â€” Flag unusual audio patterns
3. **Spatial Mapping** â€” Convert time-based scores to geographic cells
4. **Cell Aggregation** â€” Mark cells with anomalous segments

**Hyperparameters:**
- Contamination: 0.1
- n_estimators: 100
- max_samples: 256

**Output:** `anomaly_results/{transect}_anomaly_results.json`

</details>

---

### **Module 4: Archaeological Signature Recognition (DTW)** ğŸ›ï¸

<details>
<summary><b>Click to expand technical details</b></summary>

<br>

**Approach:** Build acoustic "fingerprints" of known archaeological features

**Dynamic Time Warping (DTW):**

```
Unknown Anomaly â†â†’ Reference Motif Library
        â†“
   Similarity Score
        â†“
   Classification
```

**Motif Categories:**
- ğŸº Geoglyphs
- ğŸ—ï¸ Earthworks
- ğŸ›¤ï¸ Ancient roads
- ğŸ›ï¸ Ceremonial structures

**Output:** `motif_recognition_results/{transect}_motif_recognition_results.json`

</details>

---

### **Module 5: Interactive Visualization (Folium)** ğŸ—ºï¸

<details>
<summary><b>Click to expand technical details</b></summary>

<br>

**Color-Coded System:**

| Color | Meaning | Description |
|-------|---------|-------------|
| ğŸŸ¦ Blue | Normal Terrain | Typical acoustic patterns |
| ğŸŸ§ Orange | Detected Anomaly | Unusual pattern (unclassified) |
| ğŸ”´ Red | Confirmed Site | Matched archaeological signature |

**Features:**
- ğŸ–±ï¸ Click cells for detailed analysis
- ğŸ“ GPS coordinates with confidence scores
- ğŸ”„ Toggle between analysis layers
- ğŸ’¾ Export to GeoJSON

</details>

---

## ğŸ›°ï¸ Data Sources

<div align="center">

| ğŸ“Š Source | ğŸ“ Description | ğŸ¯ Resolution | ğŸ”— Link |
|-----------|----------------|---------------|---------|
| NASA LiDAR | Ground elevation (DTMs) | 1m | [Download](https://www.kaggle.com/datasets/arnavmishra6996/nasa-amazon-lidar-2008-2018) |
| Sentinel-2 | Multi-spectral satellite | 10-20m | [Portal](https://scihub.copernicus.eu/) |
| HydroSHEDS | Hydrological flow data | 90m | [Website](https://www.hydrosheds.org/) |
| VGGish | Audio embedding model | N/A | [TF Hub](https://tfhub.dev/google/vggish/1) |

</div>

---

## ğŸ”¬ Research Impact

<div align="center">

> ### *"Archaeological features create detectable acoustic signatures when terrain data is sonified."*

</div>

**Key Innovations:**

<table>
<tr>
<td align="center" width="33%">

### ğŸ”„ Cross-Modal Translation
Geospatial â†’ Acoustic domain conversion enables new discovery methods

</td>
<td align="center" width="33%">

### ğŸ¯ Anomaly-Driven Discovery
Unsupervised detection without labeled training data

</td>
<td align="center" width="33%">

### ğŸ§¬ Interpretable Signatures
DTW pattern matching preserves archaeological context

</td>
</tr>
</table>

**Applications:**
- ğŸŒ³ Dense canopy penetration
- ğŸï¸ Large-scale landscape surveying
- ğŸ•°ï¸ Historical site preservation
- ğŸŒ Remote area exploration

---

## ğŸ“Š Results & Discoveries

<div align="center">
  <img src="./images/Screenshot%202025-10-26%20031519.png" alt="Archaeological Site Detection Results" width="750"/>
  <p><em>ğŸ—ºï¸ Interactive map showing detected archaeological anomalies (orange) and classified signatures (red) overlaid on Amazon rainforest terrain. High-confidence detections correspond to known geoglyph and earthwork locations.</em></p>
</div>

---

## âš™ï¸ Repository Structure

```
ğŸ“¦ sonar-ai/
â”‚
â”œâ”€â”€ ğŸ“œ main.py                      # End-to-end pipeline orchestration
â”œâ”€â”€ âš™ï¸  config.py                    # Configuration and hyperparameters
â”œâ”€â”€ ğŸ“Š data_loader.py               # Multi-source data loading utilities
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ ğŸµ sonification.py          # Module 1: Data-to-audio conversion
â”‚   â”œâ”€â”€ ğŸ§  vggish_embedding.py      # Module 2: Audio feature extraction
â”‚   â”œâ”€â”€ ğŸ” anomaly_detection.py     # Module 3: Isolation Forest
â”‚   â”œâ”€â”€ ğŸ¯ motif_recognition.py     # Module 4: DTW pattern matching
â”‚   â””â”€â”€ ğŸ—ºï¸  map_visualization.py     # Module 5: Interactive mapping
â”‚
â”œâ”€â”€ ğŸ“ images/                      # Visualization screenshots
â”‚   â”œâ”€â”€ Screenshot 2025-10-26 031458.png
â”‚   â””â”€â”€ Screenshot 2025-10-26 031519.png
â”‚
â”œâ”€â”€ ğŸ“ data/                        # Output directory (auto-generated)
â”‚   â”œâ”€â”€ audio_sonifications/
â”‚   â”œâ”€â”€ audio_embeddings/
â”‚   â”œâ”€â”€ anomaly_results/
â”‚   â”œâ”€â”€ motif_recognition_results/
â”‚   â””â”€â”€ visualizations/
â”‚
â”œâ”€â”€ ğŸ“‹ requirements.txt
â””â”€â”€ ğŸ“– README.md
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/arnavmishra6996/sonar-ai.git
cd sonar-ai

# Install dependencies
pip install -r requirements.txt

# Download sample data (optional)
python download_sample_data.py
```

### Requirements

```txt
tensorflow>=2.13.0
tensorflow-hub
numpy>=1.23.0
scipy>=1.10.0
librosa>=0.10.0
soundfile>=0.12.0
folium>=0.14.0
scikit-learn>=1.2.0
rasterio>=1.3.0
```

### Running the Pipeline

```bash
# Run complete pipeline
python main.py

# Run individual modules
python -m models.sonification
python -m models.vggish_embedding
python -m models.anomaly_detection
python -m models.motif_recognition
python -m models.map_visualization
```

**Pipeline Flow:**

```
Sonification â†’ Embedding â†’ Anomaly Detection â†’ Motif Recognition â†’ Visualization
    (5m)          (10m)           (2m)                (3m)              (1m)
```

---

## ğŸ§  Related Kaggle Resources

<div align="center">

| ğŸ·ï¸ Resource | ğŸ“¦ Type | ğŸ”— Link |
|-------------|---------|---------|
| ğŸŒ€ **Whispers Beneath the Canopy** | Notebook | [View â†’](https://www.kaggle.com/code/arnavmishra6996/whispers-beneath-the-canopy) |
| ğŸŒ² **NASA Amazon LiDAR 2008â€“2018** | Dataset | [Download â†’](https://www.kaggle.com/datasets/arnavmishra6996/nasa-amazon-lidar-2008-2018) |
| ğŸ’§ **HydroSHEDS South America** | Dataset | [Download â†’](https://www.kaggle.com/datasets/arnavmishra6996/south-america-hydroshed-dataset) |
| ğŸµ **Sonification + VGGish Model** | Model | [Use â†’](https://www.kaggle.com/models/arnavmishra6996/new-sonification-50km-vggish-anomaly-detect) |

</div>

---

## ğŸ“š Citation

If you use SONAR in your research, please cite:

```bibtex
@misc{mishra2025sonar,
  author       = {Mishra, Arnav},
  title        = {S.O.N.A.R: Sonic Oracle for Navigating Ancient Ruins},
  year         = {2025},
  publisher    = {Kaggle},
  journal      = {OpenAI to Z Challenge},
  howpublished = {\url{https://www.kaggle.com/code/arnavmishra6996/whispers-beneath-the-canopy}},
  note         = {AI-powered acoustic archaeology for hidden site detection}
}
```

---

## ğŸ Author

<div align="center">

### **Arnav Mishra**

*AI Researcher Â· Machine Learning & Computational Archaeology*

ğŸ“ Bhopal, India

[![Kaggle](https://img.shields.io/badge/Kaggle-Profile-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/arnavmishra6996)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/arnavmishra6996)
[![Email](https://img.shields.io/badge/Email-Contact-EA4335?style=for-the-badge&logo=gmail&logoColor=white)](mailto:your.email@example.com)

</div>

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

<details>
<summary><b>Contribution Guidelines</b></summary>

<br>

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

**Areas for Contribution:**
- ğŸµ New sonification algorithms
- ğŸ§  Alternative embedding models
- ğŸ” Improved anomaly detection
- ğŸ—ºï¸ Enhanced visualization tools
- ğŸ“Š Performance optimizations

</details>

---

## ğŸ“œ License

<div align="center">

MIT License Â© 2025 Arnav Mishra

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

*Free to use, modify, and distribute with attribution*

</div>

---

<div align="center">

### ğŸŒŸ Star this repository if you found it helpful!


---

*"Every landscape has a voice. We just need to learn how to listen."*

</div>
